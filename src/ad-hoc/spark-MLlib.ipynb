{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:00:44.343492Z",
     "start_time": "2024-03-25T09:00:44.320759Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"taxi-fare-prediction\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trips_path = '../../data/trips/*'\n",
    "trips_df = spark.read.options(inferSchema=True, header=True).format('parquet').load(trips_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:00:45.560972Z",
     "start_time": "2024-03-25T09:00:45.029185Z"
    }
   },
   "id": "10e8efced7efed2d",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: long (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: double (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: long (nullable = true)\n",
      " |-- DOLocationID: long (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "trips_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:00:46.458356Z",
     "start_time": "2024-03-25T09:00:46.454123Z"
    }
   },
   "id": "78b70234a418b144",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "trips_df.createOrReplaceTempView(\"trips\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:00:47.203818Z",
     "start_time": "2024-03-25T09:00:47.109297Z"
    }
   },
   "id": "52bb4ff6b5c9f80",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        trip_distance,\n",
    "        total_amount\n",
    "    FROM\n",
    "        trips\n",
    "    WHERE\n",
    "        total_amount < 5000\n",
    "        AND total_amount > 0\n",
    "        AND trip_distance > 0\n",
    "        AND trip_distance < 500\n",
    "        AND passenger_count < 5\n",
    "        AND TO_DATE(tpep_pickup_datetime) >= '2021-01-01'\n",
    "        AND TO_DATE(tpep_pickup_datetime) < '2021-08-01'\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:00:48.169272Z",
     "start_time": "2024-03-25T09:00:48.033909Z"
    }
   },
   "id": "39e1ecb982039b6a",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_df.createOrReplaceTempView(\"data\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:00:48.957081Z",
     "start_time": "2024-03-25T09:00:48.910243Z"
    }
   },
   "id": "64f275486731e278",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|trip_distance|total_amount|\n",
      "+-------------+------------+\n",
      "|         16.5|       70.07|\n",
      "|         1.13|       11.16|\n",
      "|         2.68|       18.59|\n",
      "|         12.4|        43.8|\n",
      "|          9.7|        32.3|\n",
      "|          9.3|       43.67|\n",
      "|         9.58|        46.1|\n",
      "|         16.2|        45.3|\n",
      "|         3.58|        19.3|\n",
      "|         0.91|        14.8|\n",
      "|         2.57|        12.8|\n",
      "|          0.4|         5.3|\n",
      "|         3.26|        17.3|\n",
      "|        13.41|       47.25|\n",
      "|         18.3|       61.42|\n",
      "|         1.53|       14.16|\n",
      "|          2.0|        11.8|\n",
      "|         16.6|       54.96|\n",
      "|         15.5|       56.25|\n",
      "|          1.3|        16.8|\n",
      "+-------------+------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:00:50.163147Z",
     "start_time": "2024-03-25T09:00:49.832427Z"
    }
   },
   "id": "5c1575c2f5f6436d",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:>                 (0 + 2) / 2][Stage 60:=========>       (6 + 5) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+\n",
      "|summary|    trip_distance|      total_amount|\n",
      "+-------+-----------------+------------------+\n",
      "|  count|         13326362|          13326362|\n",
      "|   mean|2.887408208631757|17.990672623941556|\n",
      "| stddev|3.833595726187914|13.011619004927265|\n",
      "|    min|             0.01|              0.01|\n",
      "|    max|            475.5|            4973.3|\n",
      "+-------+-----------------+------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_df.describe().show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:00:52.568652Z",
     "start_time": "2024-03-25T09:00:51.077301Z"
    }
   },
   "id": "cadae3bb063fa46c",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "## ML Split Train, Test\n",
    "\n",
    "train_df, test_df = data_df.randomSplit([0.8, 0.2], seed=6)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:00:54.526695Z",
     "start_time": "2024-03-25T09:00:54.489209Z"
    }
   },
   "id": "22c2381b42b38111",
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:>                 (0 + 2) / 2][Stage 66:=============>   (9 + 2) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000936 2665097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(trips_df.count(), test_df.count())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:01:02.466950Z",
     "start_time": "2024-03-25T09:00:57.593512Z"
    }
   },
   "id": "18cda405d288be05",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 벡터 어셈블러로 트레인이 가능한 상태로 변환한다.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vassembler = VectorAssembler(inputCols=[\"trip_distance\"], outputCol=\"features\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:01:03.575403Z",
     "start_time": "2024-03-25T09:01:03.562898Z"
    }
   },
   "id": "9929285deb14fc28",
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:>                 (0 + 2) / 2][Stage 69:>                 (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+--------+\n",
      "|trip_distance|total_amount|features|\n",
      "+-------------+------------+--------+\n",
      "|         0.01|        3.05|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "|         0.01|         3.3|  [0.01]|\n",
      "+-------------+------------+--------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "vtrain_df = vassembler.transform(train_df)\n",
    "vtrain_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:01:07.767851Z",
     "start_time": "2024-03-25T09:01:05.098885Z"
    }
   },
   "id": "9069eee80712e7d",
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/25 18:01:53 WARN Instrumentation: [1a513624] regParam is zero, which might cause numerical instability and overfitting.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 리그레션 모델 생성\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# 베이스 라인 코드\n",
    "lr = LinearRegression(\n",
    "    maxIter=50,\n",
    "    labelCol=\"total_amount\",\n",
    "    featuresCol=\"features\"\n",
    ")\n",
    "\n",
    "model = lr.fit(vtrain_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:02:26.773754Z",
     "start_time": "2024-03-25T09:01:52.275493Z"
    }
   },
   "id": "3fb758283d826b23",
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vtest_df = vassembler.transform(test_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:02:35.434986Z",
     "start_time": "2024-03-25T09:02:35.063632Z"
    }
   },
   "id": "91e47674fc6c9759",
   "execution_count": 58
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 예측\n",
    "prediction = model.transform(vtest_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:02:36.706942Z",
     "start_time": "2024-03-25T09:02:36.658342Z"
    }
   },
   "id": "fc08a855727c57b9",
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+--------+----------------+\n",
      "|trip_distance|total_amount|features|      prediction|\n",
      "+-------------+------------+--------+----------------+\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "+-------------+------------+--------+----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "prediction.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:02:39.991124Z",
     "start_time": "2024-03-25T09:02:37.610408Z"
    }
   },
   "id": "ea8a6ec3659f4aeb",
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "6.29603647659225"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.rootMeanSquaredError"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:02:41.083828Z",
     "start_time": "2024-03-25T09:02:41.079145Z"
    }
   },
   "id": "f7ed59853a8630fb",
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "0.7667360682747258"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.r2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:02:41.976048Z",
     "start_time": "2024-03-25T09:02:41.971595Z"
    }
   },
   "id": "6b87b67c48f224ec",
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|trip_distance|\n",
      "+-------------+\n",
      "|          1.1|\n",
      "|          5.5|\n",
      "|         10.5|\n",
      "|         30.0|\n",
      "+-------------+\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "distance_list = [1.1, 5.5, 10.5, 30.0]\n",
    "distance_df = spark.createDataFrame(distance_list, DoubleType()).toDF(\"trip_distance\")\n",
    "\n",
    "distance_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T08:49:24.767299Z",
     "start_time": "2024-03-25T08:49:23.536458Z"
    }
   },
   "id": "e64f872ef7cd5c1",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "vdistance_df = vassembler.transform(distance_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T08:49:33.019681Z",
     "start_time": "2024-03-25T08:49:32.959778Z"
    }
   },
   "id": "2ad7fd4b312ba5f6",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+------------------+\n",
      "|trip_distance|features|        prediction|\n",
      "+-------------+--------+------------------+\n",
      "|          1.1|   [1.1]|12.661914396267541|\n",
      "|          5.5|   [5.5]|25.781156878698354|\n",
      "|         10.5|  [10.5]| 40.68938697236974|\n",
      "|         30.0|  [30.0]| 98.83148433768814|\n",
      "+-------------+--------+------------------+\n"
     ]
    }
   ],
   "source": [
    "model.transform(vdistance_df).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T08:49:43.379297Z",
     "start_time": "2024-03-25T08:49:42.874573Z"
    }
   },
   "id": "e351ff8b03dedde",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- pickup_location_id: long (nullable = true)\n",
      " |-- dropoff_location_id: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "# 예측에 사용할 컬럼 추가, 전처리\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    passenger_count,\n",
    "    PULocationID as pickup_location_id,\n",
    "    DOLocationID as dropoff_location_id,\n",
    "    trip_distance,\n",
    "    HOUR(tpep_pickup_datetime) as pickup_time,\n",
    "    DATE_FORMAT(TO_DATE(tpep_pickup_datetime), 'EEEE') AS day_of_week,\n",
    "    total_amount\n",
    "FROM\n",
    "    trips\n",
    "WHERE\n",
    "    total_amount < 5000\n",
    "    AND total_amount > 0\n",
    "    AND trip_distance > 0\n",
    "    AND trip_distance < 500\n",
    "    AND passenger_count < 5\n",
    "    AND TO_DATE(tpep_pickup_datetime) >= '2021-01-01'\n",
    "    AND TO_DATE(tpep_pickup_datetime) < '2021-08-01'\n",
    "\"\"\"\n",
    "data_df = spark.sql(query)\n",
    "data_df.createOrReplaceTempView(\"data\")\n",
    "\n",
    "data_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:05:33.933899Z",
     "start_time": "2024-03-25T09:05:33.816474Z"
    }
   },
   "id": "d342c6e5036ec1f",
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_df, test_df = data_df.randomSplit([0.8, 0.2], seed=6)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:05:35.741913Z",
     "start_time": "2024-03-25T09:05:35.724697Z"
    }
   },
   "id": "e082628e4ba59975",
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_dir = '../../data/result/taxi/'\n",
    "\n",
    "train_df.write.format(\"parquet\").save(f\"{data_dir}/train/\")\n",
    "test_df.write.format(\"parquet\").save(f\"{data_dir}/test/\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:06:11.434767Z",
     "start_time": "2024-03-25T09:05:49.782030Z"
    }
   },
   "id": "2a11b9c191125d2",
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# read Data\n",
    "train_df = spark.read.parquet(f\"{data_dir}/train/\")\n",
    "test_df = spark.read.parquet(f\"{data_dir}/test/\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:06:18.410015Z",
     "start_time": "2024-03-25T09:06:17.156513Z"
    }
   },
   "id": "1e73f69ba61bfb8a",
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 스트링 값을 숫자값으로 바꾸어 원핫인코딩을 진행한다.\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# 카테고리 피쳐 설정\n",
    "cat_feats = [\n",
    "    \"pickup_location_id\",\n",
    "    \"dropoff_location_id\",\n",
    "    \"day_of_week\"\n",
    "]\n",
    "\n",
    "# 스테이지를 담는 배열\n",
    "stages = []\n",
    "\n",
    "\n",
    "for c in cat_feats:\n",
    "    cat_indexer = StringIndexer(inputCol=c, outputCol= c + \"_idx\").setHandleInvalid(\"keep\")\n",
    "    onehot_encoder = OneHotEncoder(inputCols=[cat_indexer.getOutputCol()], outputCols=[c + \"_onehot\"])\n",
    "    stages += [cat_indexer, onehot_encoder]\n",
    "\n",
    "\n",
    "# 숫자형 전처리: 벡터 어셈블러, 스탠다드 스칼라\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "num_feats = [\n",
    "    \"passenger_count\",\n",
    "    \"trip_distance\",\n",
    "    \"pickup_time\"\n",
    "]\n",
    "\n",
    "for n in num_feats:\n",
    "    num_assembler = VectorAssembler(inputCols=[n], outputCol= n + \"_vecotr\")\n",
    "    num_scaler = StandardScaler(inputCol=num_assembler.getOutputCol(), outputCol= n + \"_scaled\")\n",
    "    stages += [num_assembler, num_scaler]\n",
    "\n",
    "\n",
    "# 두가지를 프리프로세싱을 했는데 합치는 과정을 진행한다.\n",
    "# 벡터 어셈블러를 이용하여 가능하다.\n",
    "assembler_inputs = [c + \"_onehot\" for c in cat_feats] + [n + \"_scaled\" for n in num_feats]\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"feature_vector\")\n",
    "stages += [assembler]\n",
    "\n",
    "\n",
    "# 스테이지로 파이프 라인 생성\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "transform_stages = stages\n",
    "pipeline = Pipeline(stages=transform_stages)\n",
    "fitted_transformer = pipeline.fit(train_df)\n",
    "\n",
    "\n",
    "# 적용\n",
    "vtrain_df = fitted_transformer.transform(train_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:06:28.706898Z",
     "start_time": "2024-03-25T09:06:20.060015Z"
    }
   },
   "id": "f1cfc1d0a8a1a725",
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[StringIndexer_793927ac7def,\n OneHotEncoder_d69d3a10ce19,\n StringIndexer_85f5d974c9af,\n OneHotEncoder_f6d6b811fa69,\n StringIndexer_d27148ee81e1,\n OneHotEncoder_5b30e76edd0a,\n VectorAssembler_c139ccbbafef,\n StandardScaler_91d8c868d8c8,\n VectorAssembler_d7e57e838d8f,\n StandardScaler_dc7285ffa500,\n VectorAssembler_8acc1453c431,\n StandardScaler_e0e08579b39c,\n VectorAssembler_ccd0afec9306]"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stages"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:06:33.151592Z",
     "start_time": "2024-03-25T09:06:33.146503Z"
    }
   },
   "id": "f6621b38bc31dbf1",
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- passenger_count: double (nullable = true)\n",
      " |-- pickup_location_id: long (nullable = true)\n",
      " |-- dropoff_location_id: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_time: integer (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- pickup_location_id_idx: double (nullable = false)\n",
      " |-- pickup_location_id_onehot: vector (nullable = true)\n",
      " |-- dropoff_location_id_idx: double (nullable = false)\n",
      " |-- dropoff_location_id_onehot: vector (nullable = true)\n",
      " |-- day_of_week_idx: double (nullable = false)\n",
      " |-- day_of_week_onehot: vector (nullable = true)\n",
      " |-- passenger_count_vecotr: vector (nullable = true)\n",
      " |-- passenger_count_scaled: vector (nullable = true)\n",
      " |-- trip_distance_vecotr: vector (nullable = true)\n",
      " |-- trip_distance_scaled: vector (nullable = true)\n",
      " |-- pickup_time_vecotr: vector (nullable = true)\n",
      " |-- pickup_time_scaled: vector (nullable = true)\n",
      " |-- feature_vector: vector (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "# 모델링\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(\n",
    "    maxIter=50,\n",
    "    solver=\"normal\",\n",
    "    labelCol=\"total_amount\",\n",
    "    featuresCol=\"feature_vector\"\n",
    ")\n",
    "\n",
    "vtrain_df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:06:39.536551Z",
     "start_time": "2024-03-25T09:06:39.525586Z"
    }
   },
   "id": "d4ef65431e2e1ff1",
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/25 18:11:53 WARN Instrumentation: [6a439c81] regParam is zero, which might cause numerical instability and overfitting.\n",
      "24/03/25 18:12:14 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 3 retries)\n",
      "java.io.IOException: Connecting to /192.168.219.114:53065 failed in the last 4750 ms, fail this connection directly\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:130)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/03/25 18:12:14 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks \n",
      "java.io.IOException: Connecting to /192.168.219.114:53065 failed in the last 4750 ms, fail this connection directly\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:130)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:152)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:150)\n",
      "\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:102)\n",
      "\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1171)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1115)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1115)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1255)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/03/25 18:12:14 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks \n",
      "java.io.IOException: Connecting to /192.168.219.114:53065 failed in the last 4750 ms, fail this connection directly\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:130)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.start(RetryingBlockTransferor.java:152)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:150)\n",
      "\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:102)\n",
      "\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1171)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1115)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1115)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1255)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/03/25 18:12:14 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 3 retries)\n",
      "java.io.IOException: Failed to connect to /192.168.219.114:53065\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:294)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:130)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Operation timed out: /192.168.219.114:53065\n",
      "Caused by: java.net.ConnectException: Operation timed out\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/03/25 18:12:14 WARN BlockManager: Failed to fetch remote block taskresult_370 from [BlockManagerId(driver, 192.168.219.114, 53065, None)] after 1 fetch failures. Most recent failure cause:\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:103)\n",
      "\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1171)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1115)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1115)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1255)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.IOException: Failed to connect to /192.168.219.114:53065\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:294)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:130)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Operation timed out: /192.168.219.114:53065\n",
      "Caused by: java.net.ConnectException: Operation timed out\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/03/25 18:12:14 WARN BlockManager: Failed to fetch remote block taskresult_371 from [BlockManagerId(driver, 192.168.219.114, 53065, None)] after 1 fetch failures. Most recent failure cause:\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:322)\n",
      "\tat org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:103)\n",
      "\tat org.apache.spark.storage.BlockManager.fetchRemoteManagedBuffer(BlockManager.scala:1171)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$getRemoteBlock$8(BlockManager.scala:1115)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBlock(BlockManager.scala:1115)\n",
      "\tat org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:1255)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.$anonfun$run$1(TaskResultGetter.scala:88)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:63)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.IOException: Connecting to /192.168.219.114:53065 failed in the last 4750 ms, fail this connection directly\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:130)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\t... 1 more\n",
      "24/03/25 18:12:14 WARN TaskSetManager: Lost task 0.0 in stage 102.0 (TID 370) (192.168.219.114 executor driver): TaskResultLost (result lost from block manager)\n",
      "24/03/25 18:12:14 ERROR TaskSetManager: Task 0 in stage 102.0 failed 1 times; aborting job\n",
      "24/03/25 18:12:14 WARN TaskSetManager: Lost task 1.0 in stage 102.0 (TID 371) (192.168.219.114 executor driver): TaskResultLost (result lost from block manager)\n",
      "24/03/25 18:12:14 ERROR Instrumentation: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 102.0 failed 1 times, most recent failure: Lost task 0.0 in stage 102.0 (TID 370) (192.168.219.114 executor driver): TaskResultLost (result lost from block manager)\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2358)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1172)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1166)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1259)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1226)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1212)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1212)\n",
      "\tat org.apache.spark.ml.optim.WeightedLeastSquares.fit(WeightedLeastSquares.scala:107)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.trainWithNormal(LinearRegression.scala:456)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.$anonfun$train$1(LinearRegression.scala:354)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:329)\n",
      "\tat org.apache.spark.ml.regression.LinearRegression.train(LinearRegression.scala:186)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/03/25 18:13:34 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 1 retries)\n",
      "java.io.IOException: Connecting to /192.168.219.114:53065 failed in the last 4750 ms, fail this connection directly\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:210)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:130)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/03/25 18:13:34 ERROR RetryingBlockTransferor: Exception while beginning fetch of 1 outstanding blocks (after 1 retries)\n",
      "java.io.IOException: Failed to connect to /192.168.219.114:53065\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:294)\n",
      "\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:214)\n",
      "\tat org.apache.spark.network.netty.NettyBlockTransferService$$anon$2.createAndStart(NettyBlockTransferService.scala:130)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.transferAllOutstanding(RetryingBlockTransferor.java:173)\n",
      "\tat org.apache.spark.network.shuffle.RetryingBlockTransferor.lambda$initiateRetry$0(RetryingBlockTransferor.java:206)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: io.netty.channel.AbstractChannel$AnnotatedConnectException: Operation timed out: /192.168.219.114:53065\n",
      "Caused by: java.net.ConnectException: Operation timed out\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:337)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:776)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/seonggil/Development/bigdata/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/seonggil/Development/bigdata/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[73], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mlr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvtrain_df\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m vtest_df \u001B[38;5;241m=\u001B[39m fitted_transformer\u001B[38;5;241m.\u001B[39mtransform(test_df)\n\u001B[1;32m      5\u001B[0m predictions \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransform(vtest_df)\n",
      "File \u001B[0;32m~/Development/bigdata/spark/python/pyspark/ml/base.py:205\u001B[0m, in \u001B[0;36mEstimator.fit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    203\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_fit(dataset)\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 205\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    206\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    207\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    208\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    209\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbut got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params)\n\u001B[1;32m    210\u001B[0m     )\n",
      "File \u001B[0;32m~/Development/bigdata/spark/python/pyspark/ml/wrapper.py:381\u001B[0m, in \u001B[0;36mJavaEstimator._fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_fit\u001B[39m(\u001B[38;5;28mself\u001B[39m, dataset: DataFrame) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m JM:\n\u001B[0;32m--> 381\u001B[0m     java_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_java\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    382\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_model(java_model)\n\u001B[1;32m    383\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_copyValues(model)\n",
      "File \u001B[0;32m~/Development/bigdata/spark/python/pyspark/ml/wrapper.py:378\u001B[0m, in \u001B[0;36mJavaEstimator._fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    375\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    377\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n\u001B[0;32m--> 378\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_java_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Development/bigdata/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[1;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[1;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[0;32m~/Development/bigdata/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[0;34m(self, command, retry, binary)\u001B[0m\n\u001B[1;32m   1036\u001B[0m connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_connection()\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1038\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1039\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m binary:\n\u001B[1;32m   1040\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m response, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_connection_guard(connection)\n",
      "File \u001B[0;32m~/Development/bigdata/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001B[0m, in \u001B[0;36mClientServerConnection.send_command\u001B[0;34m(self, command)\u001B[0m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    510\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 511\u001B[0m         answer \u001B[38;5;241m=\u001B[39m smart_decode(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    512\u001B[0m         logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer received: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(answer))\n\u001B[1;32m    513\u001B[0m         \u001B[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001B[39;00m\n\u001B[1;32m    514\u001B[0m         \u001B[38;5;66;03m# answer before the socket raises an error.\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/socket.py:669\u001B[0m, in \u001B[0;36mSocketIO.readinto\u001B[0;34m(self, b)\u001B[0m\n\u001B[1;32m    667\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    668\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 669\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    670\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[1;32m    671\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timeout_occurred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = lr.fit(vtrain_df)\n",
    "\n",
    "vtest_df = fitted_transformer.transform(test_df)\n",
    "\n",
    "predictions = model.transform(vtest_df)\n",
    "\n",
    "# 캐싱\n",
    "predictions.cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:14:02.084416Z",
     "start_time": "2024-03-25T09:11:52.731264Z"
    }
   },
   "id": "3a2802f82887c42d",
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model.save(\"../../data/result/model\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:14:14.509667Z",
     "start_time": "2024-03-25T09:14:13.543192Z"
    }
   },
   "id": "5eec656e0527f707",
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 104:>                (0 + 2) / 2][Stage 112:>                (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+--------+----------------+\n",
      "|trip_distance|total_amount|features|      prediction|\n",
      "+-------------+------------+--------+----------------+\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "|         0.01|         3.3|  [0.01]|9.41192023584718|\n",
      "+-------------+------------+--------+----------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "\n",
    "\n",
    "lr_model = LinearRegressionModel().load('../../data/result/model')\n",
    "\n",
    "\n",
    "predictions = lr_model.transform(vtest_df)\n",
    "\n",
    "\n",
    "predictions.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T09:14:19.502165Z",
     "start_time": "2024-03-25T09:14:16.979975Z"
    }
   },
   "id": "771368d4ccca3781",
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f7d82bf2db5a2f56"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
